{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9A06Ln2v8zn"
      },
      "outputs": [],
      "source": [
        "# importing all the libraries\n",
        "!pip install catboost\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "import re\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from catboost import CatBoostRegressor, Pool, cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4zt8314wNso"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX3mLasQwnkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3efd997-ea8f-430f-f94b-8fad8ad820dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/cassandra/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/cassandra/\n",
        "root = \"/content/drive/MyDrive/cassandra/\"\n",
        "root = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBnAHIJywOdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "30d6f1a1-82b9-494a-8b84-0bbc60070b16"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9c66b1c62a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the training and testing data frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'trainData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'testData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainData.csv'"
          ]
        }
      ],
      "source": [
        "# load the training and testing data frames\n",
        "train_df = pd.read_csv(root + 'trainData.csv')\n",
        "test_df = pd.read_csv(root + 'testData.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df.rename(columns={\"Number_of_Days_until_Payment\":\"target\"}, inplace=True)\n",
        "#test_df.rename(columns={\"Number_of_Days_until_Payment\":\"target\"}, inplace=True)"
      ],
      "metadata": {
        "id": "ZWgXzmgvQUHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za1tP9-lwz7W"
      },
      "outputs": [],
      "source": [
        "train_df.dtypes\n",
        "# taking a look at features and their types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLkFrruh0Ifq"
      },
      "outputs": [],
      "source": [
        "print(train_df.isna().sum(), '\\n')\n",
        "print(test_df.isna().sum())\n",
        "# check if data has any NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhPBTEgwyqCv"
      },
      "outputs": [],
      "source": [
        "train_df.Description.fillna('', inplace=True)\n",
        "test_df.Description.fillna('', inplace=True)\n",
        "# samples with no Description have NaN values so we replace them with empty strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6drLrUdv1aEA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "sns.kdeplot(train_df.Number_of_Days_until_Payment)\n",
        "plt.savefig('fig4')\n",
        "# taking a look at output distribution, we notice that most target values lie within a reasonable range\n",
        "# and outliers gradually decrease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vtOc8V61gXI"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop(index = train_df[train_df.Number_of_Days_until_Payment < 0].index).reset_index(drop=True)\n",
        "# dropping the samples with negative target values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwoGH2QO4pdT"
      },
      "outputs": [],
      "source": [
        "# converting string dates into datetime format\n",
        "train_df['Invoice_Date'] = pd.to_datetime(train_df['Invoice_Date'])\n",
        "train_df['Due_Date'] = pd.to_datetime(train_df['Due_Date'])\n",
        "train_df['Created'] = pd.to_datetime(train_df['Created'])\n",
        "\n",
        "test_df['Invoice_Date'] = pd.to_datetime(test_df['Invoice_Date'])\n",
        "test_df['Due_Date'] = pd.to_datetime(test_df['Due_Date'])\n",
        "test_df['Created'] = pd.to_datetime(test_df['Created'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaAdcAkWaYOU"
      },
      "outputs": [],
      "source": [
        "train_df['Due_Invoice_delta'] = (train_df['Due_Date'] - train_df['Invoice_Date']).astype('timedelta64[D]').astype(int)\n",
        "test_df['Due_Invoice_delta'] = (test_df['Due_Date'] - test_df['Invoice_Date']).astype('timedelta64[D]').astype(int)\n",
        "# take difference of due date and invoice date to make a new feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgCEuvNFPH2h"
      },
      "outputs": [],
      "source": [
        "date_cols = ['Invoice_Date', 'Created', 'Due_Date']\n",
        "for col in date_cols:\n",
        "  train_df[col + \"_day\"] = train_df[col].dt.day.astype(str)\n",
        "  train_df[col + \"_month\"] = train_df[col].dt.month.astype(str)\n",
        "  train_df[col + \"_year\"] = train_df[col].dt.year.astype(str)\n",
        "  train_df[col + \"_weekday\"] = train_df[col].dt.weekday.astype(str)\n",
        "\n",
        "  test_df[col + \"_day\"] = test_df[col].dt.day.astype(str)\n",
        "  test_df[col + \"_month\"] = test_df[col].dt.month.astype(str)\n",
        "  test_df[col + \"_year\"] = test_df[col].dt.year.astype(str)\n",
        "  test_df[col + \"_weekday\"] = test_df[col].dt.weekday.astype(str)\n",
        "\n",
        "train_df.drop(columns=date_cols, inplace=True)\n",
        "test_df.drop(columns=date_cols, inplace=True)\n",
        "\n",
        "# split date columns into day, month, year and weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzfHRPvikz9P"
      },
      "outputs": [],
      "source": [
        "train_description= train_df['Description'].to_list()\n",
        "test_description = test_df['Description'].to_list()\n",
        "\n",
        "def text_preprocessing(\n",
        "    texts:list,\n",
        "    punctuations = r'''/!()-[]{};:'\"\\,<>./?@#$%^&*_“~''',\n",
        "    stop_words=[\"and\", \"is\", 'a', 'the', 'in', 'be', 'will','am', 'are', 'to', 'on', 'because', 'since', 'of', 'an', 'as', 'at', 'by', 'from', 'has', 'he', 'it', 'its', 'that', 'was', 'were', 'with']\n",
        "    )->list:\n",
        "  \n",
        "  final_text=[]\n",
        "  # print(text)\n",
        "  for text in texts:\n",
        "    for x in text.lower(): \n",
        "        if x in punctuations: \n",
        "            text = text.replace(x, \"  \")\n",
        "      # Setting every word to lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Converting all our text to a list \n",
        "    text = text.split(' ')\n",
        "\n",
        "    # Droping empty strings\n",
        "    text = [x for x in text if x!='']\n",
        "\n",
        "    # Droping stop words\n",
        "    text = [x for x in text if x not in stop_words]\n",
        "    final_text.append(text)\n",
        "\n",
        "  return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0XSInSrmapq"
      },
      "outputs": [],
      "source": [
        "final_description= text_preprocessing(train_description)\n",
        "final_test_description= text_preprocessing(test_description)\n",
        "\n",
        "train_df['Description'] = [\" \".join(s) for s in final_description]\n",
        "test_df['Description'] = [\" \".join(s) for s in final_test_description]\n",
        "\n",
        "document = list(train_df['Description'])\n",
        "document2 = list(test_df['Description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7J__STLthha"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a Vectorizer Object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Encode the Document\n",
        "\n",
        "new_df = list(pd.concat([train_df.drop(columns='Number_of_Days_until_Payment'), test_df], axis=0)['Description'])\n",
        "\n",
        "vectorizer.fit(new_df)\n",
        "vector = vectorizer.transform(document)\n",
        "vector = vectorizer.transform(document2)\n",
        "\n",
        "ar = np.array(vector.toarray())\n",
        "at = np.array(vector.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n23Vizy_N8nT"
      },
      "outputs": [],
      "source": [
        "# finally add numerically encoded text features in the data frames\n",
        "a = pd.DataFrame(ar)\n",
        "at = pd.DataFrame(at)\n",
        "train_df = pd.concat([train_df, a], axis=1)\n",
        "test_df = pd.concat([test_df, at], axis=1)\n",
        "\n",
        "dic = {}\n",
        "c=0\n",
        "for j, s in enumerate(set(train_df['Description'])):\n",
        "  dic[''.join(sorted(s))] = j\n",
        "  c=j\n",
        "for j, s in enumerate(set(test_df['Description'])):\n",
        "  dic[''.join(sorted(s))] = j+c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnNerv8k6GHU"
      },
      "outputs": [],
      "source": [
        "train_df['D_size'] = train_df['Description'].apply(lambda s: len(s.split()))\n",
        "test_df['D_size'] = test_df['Description'].apply(lambda s: len(s.split()))\n",
        "# we make a new feature by counting the number of words in the Description of the sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df.drop(columns='Description', inplace=True)\n",
        "#test_df.drop(columns='Description', inplace=True)"
      ],
      "metadata": {
        "id": "h0Ad9vDAg4OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Up-2G7Sg8y6"
      },
      "outputs": [],
      "source": [
        "train_df['Outstanding_zero'] = train_df['Outstanding'].apply(lambda x: (1 if x==0 else 0))\n",
        "test_df['Outstanding_zero'] = test_df['Outstanding'].apply(lambda x: (1 if x==0 else 0))\n",
        "\n",
        "# we made a new feature called “Outstandind_zero” which was 1 if the column was zero and 0 otherwise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPWEkc-Ghf40"
      },
      "outputs": [],
      "source": [
        "train_df['Outstanding_Amount_ratio'] = train_df['Outstanding']/train_df['Amount']\n",
        "train_df['Settled_Amount_ratio'] = train_df['Settled']/train_df['Amount']\n",
        "train_df['Outstanding_Settled_ratio'] = train_df['Outstanding']/train_df['Settled']\n",
        "\n",
        "test_df['Outstanding_Amount_ratio'] = test_df['Outstanding']/test_df['Amount']\n",
        "test_df['Settled_Amount_ratio'] = test_df['Settled']/test_df['Amount']\n",
        "test_df['Outstanding_Settled_ratio'] = test_df['Outstanding']/test_df['Settled']\n",
        "\n",
        "# We took ratios of these three columns to create three new features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = ['Amount', 'Settled', 'Outstanding', 'Due_Invoice_delta', 'Outstanding_zero', 'Outstanding_Amount_ratio', 'Settled_Amount_ratio', 'Outstanding_Settled_ratio']"
      ],
      "metadata": {
        "id": "oBH995mag_y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "dJMnKQcymcev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTi6sFIPmxAn"
      },
      "outputs": [],
      "source": [
        "new_df = pd.concat([train_df.drop(columns='Number_of_Days_until_Payment'), test_df], axis=0)\n",
        "for col in num_cols:\n",
        "  train_df[col + '_mean'] = train_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].mean()))\n",
        "  train_df[col + '_median'] = train_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].median()))\n",
        "  train_df[col + '_std'] = train_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].std(ddof=0)))\n",
        "  train_df[col + '_count'] = train_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].count()))\n",
        "  train_df[col + '_min'] = train_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].min()))\n",
        "  train_df[col + '_max'] = train_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].max()))\n",
        "  \n",
        "  test_df[col + '_mean'] = test_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].mean()))\n",
        "  test_df[col + '_median'] = test_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].median()))\n",
        "  test_df[col + '_std'] = test_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].std(ddof=0)))\n",
        "  test_df[col + '_count'] = test_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].count()))\n",
        "  test_df[col + '_min'] = test_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].min()))\n",
        "  test_df[col + '_max'] = test_df['Vendor_Name'].map(dict(new_df.groupby('Vendor_Name')[col].max()))\n",
        "\n",
        "# we took mean, median, minimum, maximum, stddev and count of target for each unique Vendor and made these into new features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "sns.kdeplot(train_df[train_df['Amount'] < 1e3]['Amount'], train_df['Number_of_Days_until_Payment'])\n",
        "plt.savefig('fig1')"
      ],
      "metadata": {
        "id": "KaxXwbWPRPr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['Monday', 'Tuesday', 'Wednusday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "y= train_df.groupby('Invoice_Date_weekday')['Number_of_Days_until_Payment'].median().values\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.barplot(x,y)\n",
        "plt.ylabel('Number_of_Days_until_Payment')\n",
        "plt.xlabel('Invoice Weekday')\n",
        "plt.savefig('fig2')"
      ],
      "metadata": {
        "id": "9vuJhRntVu47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['Jan', 'Feb', 'Mar', 'Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
        "y = train_df.groupby('Invoice_Date_month')['Number_of_Days_until_Payment'].median().values\n",
        "plt.figure(figsize=(16,10))\n",
        "sns.barplot(x,y)"
      ],
      "metadata": {
        "id": "bQQQTAUcYRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "sns.scatterplot(train_df['Due_Invoice_delta'], train_df['Number_of_Days_until_Payment'], color='red')\n",
        "plt.savefig('fig3')"
      ],
      "metadata": {
        "id": "gccuQU_jZxfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['']"
      ],
      "metadata": {
        "id": "FBXj_DNYcYfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqDuYtRzvb9a"
      },
      "outputs": [],
      "source": [
        "x_train = np.array(train_df.drop(columns='Number_of_Days_until_Payment'))\n",
        "y_train = np.array(train_df['Number_of_Days_until_Payment'])\n",
        "x_test = np.array(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38ufhCBypNwz"
      },
      "outputs": [],
      "source": [
        "# training the CatBoost Regressor\n",
        "date_cols = ['Invoice_Date', 'Created', 'Due_Date']\n",
        "a = []\n",
        "for col in date_cols:\n",
        "    a.append(col + \"_day\")\n",
        "    a.append(col + \"_month\")\n",
        "\n",
        "    a.append(col + \"_year\")\n",
        "\n",
        "    a.append(col + \"_weekday\")\n",
        "\n",
        "a.append(\"Vendor_Name\")\n",
        "\n",
        "pool_train = Pool(train_df.drop(columns=['Number_of_Days_until_Payment']), train_df['Number_of_Days_until_Payment'],\n",
        "                  cat_features = a)\n",
        "model = CatBoostRegressor(verbose=0)\n",
        "model.fit(pool_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ2YKXjfwC1S"
      },
      "outputs": [],
      "source": [
        "print(cv(pool_train, model.get_all_params(),fold_count=5, plot=True))\n",
        "# k fold cross validation to evaluate our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B1bi9MCwcxy"
      },
      "outputs": [],
      "source": [
        "np.sqrt(np.mean((y_train - model.predict(x_train))**2))\n",
        "# checking the RMSE score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoT4Br8-0HPP"
      },
      "outputs": [],
      "source": [
        "# generating final predictions\n",
        "pred = model.predict(test_df)\n",
        "out = pd.DataFrame([pd.read_csv(root + 'testData.csv')['Vendor_Name'], pred]).transpose()\n",
        "out.columns = ['Vendor_Name', 'Number_of_Days_until_Payment']\n",
        "out.to_csv('out1.csv', index=False)\n",
        "pd.read_csv('out1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tYUmsg5LmAZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Final cassandra.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}